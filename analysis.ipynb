{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move review sentiment analysis with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mateusz Pabian\n",
    "\n",
    "The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis. The aim of this task is to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, language ambiguity, and many others make this task very challenging with the best Kaggle public leaderboard score of 76.53% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Rotten Tomatoes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using Pandas DataFrame API\n",
    "train = pd.read_csv('train.tsv', sep='\\t', header = 0)\n",
    "test = pd.read_csv('test.tsv', sep='\\t', header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation. Words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "Word embeddings is a popular model used in natural language processing, usually as features for deep learning of recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_used = 2000\n",
    "\n",
    "# tokenize reviews\n",
    "tokenizer = Tokenizer(num_words = words_used, split=' ')\n",
    "tokenizer.fit_on_texts(train['Phrase'].values)\n",
    "\n",
    "# obtain numerical representation of sentences\n",
    "train_data = tokenizer.texts_to_sequences(train['Phrase'].values)\n",
    "test_data = tokenizer.texts_to_sequences(test['Phrase'].values)\n",
    "\n",
    "# reshape data\n",
    "train_data_re = pad_sequences(train_data)\n",
    "test_data_re = pad_sequences(test_data)\n",
    "\n",
    "# obtain one-hot encoding of class labels\n",
    "labels = pd.get_dummies(train['Sentiment']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is a recurrent neural network architecture commonly used for time series analysis. \n",
    "\n",
    "It is designed to handle both short- and long-term dependencies in the signal due to it's memory units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46, 64)            128000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 227,461\n",
      "Trainable params: 227,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(words_used, 64, input_length=46))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "156060/156060 [==============================] - 326s 2ms/step - loss: 1.0803 - acc: 0.5735\n",
      "Epoch 2/4\n",
      "156060/156060 [==============================] - 309s 2ms/step - loss: 0.9764 - acc: 0.6153\n",
      "Epoch 3/4\n",
      "156060/156060 [==============================] - 321s 2ms/step - loss: 0.9549 - acc: 0.6250\n",
      "Epoch 4/4\n",
      "156060/156060 [==============================] - 309s 2ms/step - loss: 0.9412 - acc: 0.6315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159fdef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_re, labels, epochs=4, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction to this assignment, Rotten Tomatoes review sentiment challenge is a difficult task with best results on Kaggle measured in the mid-70%. It is difficult to evaluate model performance because test dataset labels are not available. It is important to note that due to length of time required to train each epoch (over 300 seconds), no network fine-tuning was performed. It seems that a use of GPU or distributed computing is necessary for the task at hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
